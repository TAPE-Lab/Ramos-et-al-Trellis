{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f8395d-3d6e-41fa-a73f-521791c513a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Figure 5a: PDO 11 coculture by treatment/culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e9e0e-0cfd-44e7-96ca-351623b34f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scprep\n",
    "import phate\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from MultiscaleEMD import MetricTree\n",
    "import sklearn\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data_path = \"/Users/mariaramos/Dropbox/Merged_files/\"\n",
    "#!ls -lah $data_path\n",
    "file_name = \"Metadata_final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ddd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load metadata from file\n",
    "raw_df = pd.read_pickle(data_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Variable: select for specific subsets of the data\"\"\"\n",
    "\n",
    "cell_type_mask = ((raw_df['Cell_type'] == 'PDOs'))\n",
    "culture_mask = ((raw_df['Culture'] == 'PDO'))\n",
    "patient_mask = ((raw_df['Patient'] == '11'))\n",
    "batch_mask = ((raw_df['Batch'] == 1))\n",
    "plate_mask = ((raw_df['Plate'] == 'SLV'))\n",
    "\n",
    "data_masked = raw_df.loc[patient_mask & cell_type_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c17901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean-up: removing pPKCa to remove NaN values - downsample \n",
    "\n",
    "data_masked.drop('pPKCa', axis=1, inplace=True)\n",
    "# data_masked = data_masked.sample(n=1000000, random_state=1, replace=False)\n",
    "data_masked.index = np.arange (data_masked.shape[0])\n",
    "\n",
    "#Creates a tag for normalization that includes Patient + Date\n",
    "def col_join(arr):\n",
    "    \"\"\" Joins columns of a dataframe into a single column with underscores \"\"\"\n",
    "    return \"_\".join(np.array(arr).astype(\"str\"))\n",
    "    \n",
    "norm_tags = data_masked.iloc[:, -9:-7].T.apply(col_join).rename(\"Norm_tag\")\n",
    "df_with_tags = pd.concat([data_masked, norm_tags], axis=1)\n",
    "\n",
    "#We need to split the data to normalise the numerical columns\n",
    "data_for_norm = df_with_tags.iloc[:, :-10]\n",
    "non_numerical = df_with_tags.iloc[:, -10:-1]\n",
    "\n",
    "# Arcsinh transformation and batch normalisation\n",
    "data_arcs = np.arcsinh(data_for_norm / 5)\n",
    "data_centered = scprep.normalize.batch_mean_center(\n",
    "    data_arcs.copy(), sample_idx=df_with_tags[\"Norm_tag\"]\n",
    ")\n",
    "\n",
    "#Re-merge the non-numerical values to the normalised data\n",
    "full_centered = pd.concat([data_centered, non_numerical], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e69d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a canonical list of names for each sample so we can order by this in\n",
    "# all subsequent processing\n",
    "\n",
    "full_names = full_centered.iloc[:, -9:].T.apply(col_join).rename(\"Full_name\")\n",
    "df_with_names = pd.concat([full_centered, full_names], axis=1)\n",
    "\n",
    "metadata = df_with_names.iloc[:, -10:]\n",
    "data = full_centered.iloc[:, :-9]\n",
    "\n",
    "full_data = pd.concat([metadata, data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49656c-2cac-42b1-b073-ec4a5138d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_locs = (\n",
    "    df_with_names.groupby(\n",
    "        [\n",
    "            \"Culture\",\n",
    "            \"Date\",\n",
    "            \"Treatment\",\n",
    "            \"Concentration\",\n",
    "            \"Replicate\",\n",
    "            \"Cell_type\",\n",
    "            \"Patient\",\n",
    "            \"Full_name\",\n",
    "            \"Batch\", \n",
    "            \"Plate\"\n",
    "        ]\n",
    "    )\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "meta = mean_locs.index.to_frame()\n",
    "\n",
    "# Sorting is necessary so that np.unique works for labeling the samples on the tree\n",
    "# meta contains one line per sample and the metadata table associated with that sample\n",
    "meta = meta.reset_index(drop=True).sort_values(\"Full_name\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f5f166-6b84-41fd-9511-77c81042171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sig_markers = [\n",
    "    \"pHH3\",\n",
    "    \"RFP\",\n",
    "    \"mCHERRY\",\n",
    "    \"Vimentin\",\n",
    "    \"EpCAM\",\n",
    "    \"CK18\",\n",
    "    \"Pan_CK\",\n",
    "    \"GFP\",\n",
    "    \"IdU\",\n",
    "    \"cCaspase_3\",\n",
    "    \"Geminin\",\n",
    "    \"pRB\",\n",
    "    \"PLK\",\n",
    "    \"CHGA\",\n",
    "    \"CD90\",\n",
    "    \"cPARP\",\n",
    "    \"Cyclin_B1\",\n",
    "]\n",
    "data_sig = data.drop(\n",
    "    non_sig_markers,\n",
    "    axis=1,\n",
    "    inplace=False,\n",
    ")\n",
    "#Cell_type markers or problematic antibodies (cCaspase_3)\n",
    "cell_type_markers = [\n",
    "    \"RFP\",\n",
    "    \"mCHERRY\",\n",
    "    \"Vimentin\",\n",
    "    \"EpCAM\",\n",
    "    \"CK18\",\n",
    "    \"Pan_CK\",\n",
    "    \"GFP\",\n",
    "    \"CHGA\",\n",
    "    \"CD90\",\n",
    "    \"cCaspase_3\"\n",
    "]\n",
    "\n",
    "\n",
    "data_all = data.drop(\n",
    "    cell_type_markers,\n",
    "    axis=1,\n",
    "    inplace=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74184584-5224-4f57-8e07-da5ad70509de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a subtree for each leaf phase\n",
    "unique, inverse = np.unique(full_names, return_inverse=True)\n",
    "assert np.array_equal(unique, meta[\"Full_name\"].values)\n",
    "\n",
    "thresholds = {\n",
    "    \"20211116_11\": {\n",
    "        \"pRB\": 1,\n",
    "        \"IdU\": 2,\n",
    "        \"pHH3\": 3,\n",
    "        \"Cyclin_B1\": -0.7,\n",
    "        \"cPARP\": 0,\n",
    "        \"pHistone_H2A\":-0.6\n",
    "    },\n",
    "    \"20210608_11\": {\n",
    "        \"pRB\": 0.2,\n",
    "        \"IdU\": 1.8,\n",
    "        \"pHH3\": 3,\n",
    "        \"Cyclin_B1\": -1.5,\n",
    "        \"cPARP\": 0,\n",
    "        \"pHistone_H2A\":-0.6\n",
    "\n",
    "    },\n",
    "    \"20210607_11\": {\n",
    "        \"pRB\": 0.2,\n",
    "        \"IdU\": 1.8,\n",
    "        \"pHH3\": 3,\n",
    "        \"Cyclin_B1\": -1.5,\n",
    "        \"cPARP\": 0,\n",
    "        \"pHistone_H2A\":-0.6\n",
    "\n",
    "    },\n",
    "}\n",
    "\n",
    "discretes = []\n",
    "for norm_tag in df_with_tags[\"Norm_tag\"].unique():\n",
    "    mask = df_with_tags[\"Norm_tag\"] == norm_tag\n",
    "    discretes.append(\n",
    "        pd.concat(\n",
    "            [data_centered[mask][gene] > thresh for gene, thresh in thresholds[norm_tag].items()],\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "discrete = pd.concat(discretes, axis=0).reindex(data.index)\n",
    "\n",
    "tree = {\n",
    "    \"S_phase_H2Ahi\": discrete[\"pRB\"] & discrete[\"IdU\"] & discrete[\"pHistone_H2A\"],\n",
    "    \"S_phase_H2Alo\": discrete[\"pRB\"] & discrete[\"IdU\"] & ~discrete[\"pHistone_H2A\"],\n",
    "    \"M_phase_H2Ahi\": discrete[\"pRB\"] & ~discrete[\"IdU\"] & discrete[\"pHH3\"]\n",
    "     & discrete[\"pHistone_H2A\"],\n",
    "    \"M_phase_H2Alo\": discrete[\"pRB\"] & ~discrete[\"IdU\"] & discrete[\"pHH3\"]\n",
    "     & ~discrete[\"pHistone_H2A\"],\n",
    "    \"G2_phase_H2Ahi\": discrete[\"pRB\"] & ~discrete[\"IdU\"] & ~discrete[\"pHH3\"]\n",
    "    & discrete[\"Cyclin_B1\"] & discrete[\"pHistone_H2A\"],\n",
    "    \"G2_phase_H2Alo\": discrete[\"pRB\"] & ~discrete[\"IdU\"] & ~discrete[\"pHH3\"]\n",
    "    & discrete[\"Cyclin_B1\"] & ~discrete[\"pHistone_H2A\"],\n",
    "    \"G1_phase_H2Ahi\": discrete[\"pRB\"] & ~discrete[\"IdU\"] & ~discrete[\"pHH3\"]\n",
    "    & ~discrete[\"Cyclin_B1\"] & discrete[\"pHistone_H2A\"],\n",
    "    \"G1_phase_H2Alo\": discrete[\"pRB\"] & ~discrete[\"IdU\"] & ~discrete[\"pHH3\"]\n",
    "    & ~discrete[\"Cyclin_B1\"] & ~discrete[\"pHistone_H2A\"],\n",
    "     \"Apoptosis_H2Ahi\": ~discrete[\"pRB\"] & discrete[\"cPARP\"] \n",
    "    & discrete[\"pHistone_H2A\"],\n",
    "    \"Apoptosis_H2Alo\": ~discrete[\"pRB\"] & discrete[\"cPARP\"] \n",
    "    & ~discrete[\"pHistone_H2A\"],\n",
    "    \"G0_phase_H2Ahi\": ~discrete[\"pRB\"] & ~discrete[\"cPARP\"] \n",
    "    & discrete[\"pHistone_H2A\"],\n",
    "    \"G0_phase_H2Alo\": ~discrete[\"pRB\"] & ~discrete[\"cPARP\"] \n",
    "    & ~discrete[\"pHistone_H2A\"],\n",
    "}\n",
    "\n",
    "df_tree = pd.DataFrame(tree)\n",
    "leaf_phases = [\"S_phase_H2Ahi\", \"S_phase_H2Alo\", \"M_phase_H2Ahi\", \"M_phase_H2Alo\",\n",
    "\"G2_phase_H2Ahi\", \"G2_phase_H2Alo\", \"G1_phase_H2Ahi\", \"G1_phase_H2Alo\", \"Apoptosis_H2Ahi\", \"Apoptosis_H2Alo\", \"G0_phase_H2Ahi\", \"G0_phase_H2Alo\"\n",
    "]\n",
    "\n",
    "proportions = (\n",
    "    pd.concat([metadata, df_tree], axis=1)\n",
    "    .groupby(\n",
    "        [\n",
    "            \"Culture\",\n",
    "            \"Date\",\n",
    "            \"Treatment\",\n",
    "            \"Concentration\",\n",
    "            \"Replicate\",\n",
    "            \"Cell_type\",\n",
    "            \"Patient\",\n",
    "            \"Full_name\",\n",
    "        ]\n",
    "    )\n",
    "    .mean()\n",
    "    .sort_values(\"Full_name\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Encodes\n",
    "onehot = OneHotEncoder(sparse=True)\n",
    "labels = onehot.fit_transform(inverse.reshape(-1, 1))\n",
    "\n",
    "# TODO this has changed in v6\n",
    "d = np.array(labels.sum(axis=0)).flatten()\n",
    "labels_normed = labels.tocoo()\n",
    "labels_normed.data = labels_normed.data / d[labels_normed.col]\n",
    "labels_normed = labels_normed.tocsr()\n",
    "\n",
    "\n",
    "def l1_embeddings(cts, edge_weights):\n",
    "    return np.array(\n",
    "        [np.asarray(cts)[i, :] * np.asarray(edge_weights) for i in range(len(cts))]\n",
    "    )\n",
    "\n",
    "\n",
    "def leaf_runner(\n",
    "    data, labels, tree_type, n_trees, norm_per_subtree=False, random_state=42, **kwargs\n",
    "):\n",
    "    \"\"\"Creates tree embeddings for each sample based on tree parameters.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        data: [# cells x # features] data matrix\n",
    "        labels: [# cells x # distributions] (potentially sparse) describing membership of cells to distributions\n",
    "        tree_type: type of tree to build over the features\n",
    "        n_trees: how many trees to build\n",
    "        norm_per_subtree: whether to treat each subtree as a separate distribution,\n",
    "                          this essentially weights each subtree equally, rather than weighting\n",
    "                          based on how many cells are in each subtree (default)\n",
    "\n",
    "    Returns:\n",
    "        leaf_embeds: [# distributions x (n_trees x n_nodes)] embeddings one per distribution where L1 distrance\n",
    "                     between embeddings represents tree EMD\n",
    "        leaf_trees: Tree objects for each tree\n",
    "        leaf_ids: Leaf label for each tree node [n_nodes] containing the strings of the leaf_phases\n",
    "    \"\"\"\n",
    "    leaf_embeds = []\n",
    "    leaf_trees = []\n",
    "    leaf_ids = []\n",
    "    # note that we only build a tree for each leaf phase leaving out proliferating vs. not\n",
    "    rs = random_state\n",
    "    for leaf in leaf_phases:\n",
    "        mask = np.array(df_tree[leaf])\n",
    "        sub_data = data[mask]\n",
    "        sub_labels = labels[mask]\n",
    "        if norm_per_subtree:\n",
    "            d = np.array(sub_labels.sum(axis=0)).flatten()\n",
    "            # Fix divide by zero errors\n",
    "            d = np.clip(d, a_min=1e-8, a_max=None)\n",
    "            sub_labels = sub_labels.tocoo()\n",
    "            sub_labels.data = sub_labels.data / (d[sub_labels.col])\n",
    "            sub_labels = sub_labels.tocsr()\n",
    "        embeds = []\n",
    "        mts = []\n",
    "        for i in range(n_trees):\n",
    "            mt = MetricTree(tree_type=tree_type, random_state=rs, **kwargs)\n",
    "            counts, edge_weights = mt.fit_transform(\n",
    "                X=sub_data,\n",
    "                y=sub_labels,\n",
    "            )\n",
    "            embeds.extend(l1_embeddings(counts.todense(), edge_weights).T)\n",
    "            mts.append(mt)\n",
    "        embeds = np.array(embeds).T\n",
    "        leaf_embeds.append(embeds)\n",
    "        leaf_trees.append(mts)\n",
    "        leaf_ids.append([leaf] * embeds.shape[1])\n",
    "        rs += 1\n",
    "    leaf_embeds = np.concatenate(leaf_embeds, axis=1)\n",
    "    leaf_ids = np.concatenate(leaf_ids)\n",
    "    return leaf_embeds, leaf_trees, leaf_ids\n",
    "\n",
    "\n",
    "def tree_runner(data, labels, tree_type, n_trees, random_state=42, **kwargs):\n",
    "    \"\"\"Creates tree embeddings for each sample based on tree parameters.\n",
    "\n",
    "    This ignores known cell state structure and simply builds a tree over the entire dataset.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        data: [# cells x # features] data matrix\n",
    "        labels: [# cells x # distributions] (potentially sparse) describing membership of cells to distributions\n",
    "        tree_type: type of tree to build over the features\n",
    "        n_trees: how many trees to build\n",
    "        norm_per_subtree: whether to treat each subtree as a separate distribution,\n",
    "                          this essentially weights each subtree equally, rather than weighting\n",
    "                          based on how many cells are in each subtree (default)\n",
    "\n",
    "    Returns:\n",
    "        leaf_embeds: [# distributions x (n_trees x n_nodes)] embeddings one per distribution where L1 distrance\n",
    "                     between embeddings represents tree EMD\n",
    "        leaf_trees: Tree objects for each tree\n",
    "    \"\"\"\n",
    "    embeds = []\n",
    "    mts = []\n",
    "    for i in range(n_trees):\n",
    "        mt = MetricTree(tree_type=tree_type, random_state=random_state + i, **kwargs)\n",
    "        counts, edge_weights = mt.fit_transform(\n",
    "            X=data,\n",
    "            y=labels,\n",
    "        )\n",
    "        embeds.extend(l1_embeddings(counts.todense(), edge_weights).T)\n",
    "        mts.append(mt)\n",
    "    embeds = np.array(embeds).T\n",
    "    return embeds, mts\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739c380-5f48-4f7e-bd61-d705aa7473fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {\n",
    "    \"nonorm\": leaf_runner(\n",
    "        data_sig,\n",
    "        labels_normed,\n",
    "        \"cluster\",\n",
    "        10,\n",
    "        norm_per_subtree=False,\n",
    "        n_levels=4,\n",
    "        n_clusters=4,\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda73304",
   "metadata": {},
   "source": [
    "# Pairwise normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70543a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_embeds(run):\n",
    "    index = pd.MultiIndex.from_frame(\n",
    "        meta.loc[:, [\"Patient\", \"Concentration\", \"Culture\", \"Replicate\", \"Treatment\", \"Plate\"]]\n",
    "    )\n",
    "    indexed_run = pd.DataFrame(run, index=index)\n",
    "    control_run = (\n",
    "        indexed_run.xs(\"0\", level=\"Concentration\").groupby([\"Patient\", \"Culture\", \"Plate\"]).mean()\n",
    "    )\n",
    "    diff = indexed_run - control_run\n",
    "    diff = pd.DataFrame(index=index).join(diff)\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = {'11':'#da70d6', '21': '#9acd32', '23':'#cd5c5c', '27':'#7d0f0f', '75':'#808000', \n",
    "            '99':'#73B0E9', '109':'#3b65a8', '141':'#663399', '216':'#8b008b', '5': '#d42f81'}\n",
    "culture = {'PDO':'#008C26', 'PDOF':'#FF7F7F', 'F':'blue'}\n",
    "treatment = {'DMSO':'#000000', 'H2O':'#000000', 'AH':'#000000', 'S':'#0433FF', 'VS':'#011993', 'L':'#F2AE40', 'F':'#942193', \n",
    "             'C':'#B7933A', 'CS':'#005493', 'CSF':'#0096FF', 'SF':'#7A81FF', 'V':'#FFD479', 'CF':'#941751', 'O':'#38774F'}\n",
    "concentration = {'0':60, '1':100, '2':220, '3':340, '4':460, '5':540}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncentered_embeds = get_dist_embeds(runs[\"nonorm\"][0])\n",
    "\n",
    "phate_op = phate.PHATE(\n",
    "    random_state=42,\n",
    "    n_jobs=-2,\n",
    "    knn_dist=\"manhattan\",\n",
    "    n_pca=None,\n",
    "    verbose=False,\n",
    ")\n",
    "phate_coords_funct = phate_op.fit_transform(uncentered_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b877eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(15,10))\n",
    "plt.tick_params(left = False, right = False , labelleft = False ,\n",
    "                labelbottom = False, bottom = False)\n",
    "\n",
    "plt.scatter(phate_coords_funct[:,0], phate_coords_funct[:,1], c=meta['Treatment'].map(treatment),\n",
    "    s=meta['Concentration'].map(concentration), alpha = .75)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b877eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(15,10))\n",
    "plt.tick_params(left = False, right = False , labelleft = False ,\n",
    "                labelbottom = False, bottom = False)\n",
    "\n",
    "plt.scatter(phate_coords_funct[:,0], phate_coords_funct[:,1], c=meta['Culture'].map(culture),\n",
    "    s=meta['Concentration'].map(concentration), alpha = .75)\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
